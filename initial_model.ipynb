{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "USE_GPU = True\n",
    "from model import *\n",
    "from data_loaders import load_mnist\n",
    "from tools import weights_init_xavier\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as path\n",
    "SAVE_DIR = \"saved_models\"\n",
    "FILENAME = \"model63.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(capsnet, initializer):\n",
    "  capsnet.conv_layer.conv.apply(initializer)\n",
    "  capsnet.primary_capsules.apply(initializer)\n",
    "  capsnet.decoder.apply(initializer)\n",
    "  #nn.init.xavier_normal_(capsnet.digit_caps.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsnet = CapsNet()\n",
    "if USE_GPU:\n",
    "  capsnet.cuda()\n",
    "optimizer = torch.optim.Adam(capsnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model found\n"
     ]
    }
   ],
   "source": [
    "filepath = path.join(SAVE_DIR, FILENAME)\n",
    "if path.isfile(filepath):\n",
    "  print(\"Saved model found\")\n",
    "  capsnet.load_state_dict(torch.load(filepath))\n",
    "else:\n",
    "  print(\"Saved model not found; Model initialized.\")\n",
    "  capsnet.initialize_weights(weights_init_xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters\"\"\"\n",
    "max_epochs = 1000\n",
    "batch_size = 128\n",
    "train_loader, test_loader = load_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-965e282f7cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mcapsnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "Te_LOSS = []\n",
    "Tr_LOSS = []\n",
    "test_acc = []\n",
    "display_step = 450\n",
    "for epoch in range(max_epochs):\n",
    "  capsnet.train()\n",
    "  train_loss = 0\n",
    "  for batch, (data, target) in list(enumerate(train_loader)):\n",
    "    target = torch.eye(10).index_select(dim=0, index=target)\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    \n",
    "    if USE_GPU:\n",
    "      data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output, reconstructions, masked = capsnet(data, target)\n",
    "    loss = capsnet.loss(data, target, output, reconstructions)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss += loss.data.item()\n",
    "    if batch % display_step == 0 and batch != 0:\n",
    "      eval_time = time.time()\n",
    "\n",
    "      capsnet.eval()\n",
    "      test_loss = 0\n",
    "      test_correct = 0\n",
    "      test_total = 0\n",
    "      for batch_id, (data, target) in enumerate(test_loader):\n",
    "        target = torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_GPU:\n",
    "          data,target = data.cuda(), target.cuda()\n",
    "\n",
    "        output, reconstruction, masked = capsnet(data)\n",
    "        loss = capsnet.loss(data, target, output, reconstruction)\n",
    "\n",
    "        test_loss += loss.data.item()\n",
    "        test_total += data.size(0)\n",
    "        test_correct += sum(np.argmax(masked.data.cpu().numpy(),1 ) == np.argmax(target.data.cpu().numpy(), 1))\n",
    "\n",
    "      acc = test_correct / test_total\n",
    "      Te_LOSS.append(test_loss / len(test_loader))\n",
    "      Tr_LOSS.append(train_loss / len(train_loader))\n",
    "      test_acc.append(acc)\n",
    "      test_loss /= len(test_loader)\n",
    "      train_loss /= len(train_loader)\n",
    "      time_spent = time.time() - t\n",
    "      t = time.time()\n",
    "      \n",
    "      print(\"Epoch: {:3.0f} \\t Time: {:3.0f} \\t Test: {:.3f} \\t Train: {:.3f} \\t Accuracy: {:3.4f}\".format(epoch, time_spent,test_loss, train_loss, acc*100))\n",
    "      train_loss = 0\n",
    "      filepath = path.join(SAVE_DIR, \"model{}.pt\".format(epoch))\n",
    "      torch.save(capsnet.state_dict(), filepath)\n",
    "      capsnet.train()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsnet.eval()\n",
    "data, target = iter(test_loader).next()\n",
    "output, reconstruction, masked = capsnet(data.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.max((output**2).sum(dim=2).squeeze(), dim=1)[1].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb5a236b8d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 3\n",
    "print(target[i], predictions[i])\n",
    "im = reconstruction[i,0].data.cpu().numpy()\n",
    "im += abs(im.min())\n",
    "im /= im.max()\n",
    "plt.imshow(im, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
